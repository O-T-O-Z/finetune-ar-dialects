{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn matplotlib pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1: The Effect of Train Size MSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"results/ex_trainsize/\"\n",
    "\n",
    "data = {}\n",
    "for f in os.listdir(root):\n",
    "    if not f.endswith(\".json\"):\n",
    "        continue\n",
    "    name = f.split(\"results_common_voice_\")[1].split(\".json\")[0]\n",
    "    with open(os.path.join(root, f), \"r\") as file:\n",
    "        data[name] = json.load(file)\n",
    "\n",
    "df_trainsize = pd.DataFrame(data).drop([\"eval_samples_per_second\", \"eval_steps_per_second\", \"eval_runtime\", \"eval_loss\"])\n",
    "df_trainsize = df_trainsize[[\"whisper-small\", \"0.2\", \"0.4\", \"0.6\", \"0.8\", \"1.0\", \"whisper-large-v3\"]].T\n",
    "df_trainsize.rename(index={\"whisper-small\": \"whisper-small (0%)\", \"0.2\": \"20%\", \"0.4\": \"40%\", \"0.6\": \"60%\", \"0.8\": \"80%\", \"1.0\": \"100%\"}, inplace=True)\n",
    "df_trainsize.rename(columns={\"eval_wer\": \"WER\", \"eval_cer\": \"CER\"}, inplace=True)\n",
    "df_trainsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the data in a bar plot with each row as a separate bar\n",
    "df_trainsize.plot(kind='bar', stacked=True, figsize=(10, 5), rot=0, colormap='Set2')\n",
    "plt.title(\"Model performance on MSA dataset with different training sizes\", fontweight='bold')\n",
    "plt.ylabel(\"Percentage\")\n",
    "plt.xlabel(\"Percentage of MSA training data used\")\n",
    "plt.legend(title=\"Metric\")\n",
    "plt.grid(axis='y')\n",
    "plt.yticks(np.arange(0, 101, 10))\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"results/plots/ex_trainsize.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2: Comparison with and without pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_results(root, prefix):\n",
    "    data = {}\n",
    "    dfs = []\n",
    "    files = os.listdir(root)\n",
    "    files.sort()\n",
    "    for f in files:\n",
    "        if not f.endswith(\".json\"):\n",
    "            continue\n",
    "        name = f.split(f\"results_whisper-small-{prefix}_\")[1].split(\".json\")[0]\n",
    "        with open(os.path.join(root, f), \"r\") as file:\n",
    "            data[name] = json.load(file)\n",
    "        df = pd.DataFrame(data[name]).drop([\"eval_samples_per_second\", \"eval_steps_per_second\", \"eval_runtime\", \"eval_loss\"])\n",
    "        df[\"model\"] = name.title()\n",
    "        df.columns = df.columns.str.title()\n",
    "        df.rename(columns={\"Msa\": \"MSA\"}, inplace=True)\n",
    "        dfs.append(df)\n",
    "\n",
    "    df = pd.concat(dfs).T\n",
    "    df.columns = pd.MultiIndex.from_tuples([tuple(c.split(\"_\")) for c in df.columns])\n",
    "    df.columns = df.columns.droplevel(0)\n",
    "    df.columns = pd.MultiIndex.from_tuples([(df.iloc[-1].iloc[i], c) for i, c in enumerate(df.columns)], names=[\"model\", \"metric\"])\n",
    "    df = df.drop(df.index[-1])\n",
    "    df = df.T\n",
    "    df_wer = df.xs(\"wer\", level=\"metric\")\n",
    "    df_cer = df.xs(\"cer\", level=\"metric\")\n",
    "    df = pd.concat([df_wer, df_cer], keys=[\"WER\", \"CER\"], names=[\"metric\"])\n",
    "    df = df.astype(float).round(2)\n",
    "    df = df.reindex([(\"WER\", \"All\")] + [(\"WER\", k) for k in list(df.index.levels[1]) if k != \"All\"] + [(\"CER\", \"All\")] + [(\"CER\", k) for k in list(df.index.levels[1]) if k != \"All\"])\n",
    "    df.to_latex(f\"{root}.tex\", multirow=True, multicolumn=True, multicolumn_format=\"c\", bold_rows=True, caption=\"Model performance on each test set\", label=f\"tab:{root.split('/')[1]}\", float_format=\"%.2f\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scratch = load_results(\"results/ex_scratch\", \"dialect\")\n",
    "df_finetune = load_results(\"results/ex_finetune\", \"finetune\")\n",
    "df_full = pd.concat([df_finetune, df_scratch], keys=[\"With\", \"Without\"], names=[\"Pre-Training\"])\n",
    "df_full.index.names = [\"Pre-Training\", \"Metric\", \"Train set\"]\n",
    "df_full.to_latex(\"results/table.tex\", multirow=True, multicolumn=True, multicolumn_format=\"c\", bold_rows=True, caption=\"Model performance on each test set without pre-training\", label=\"tab:all_res\", float_format=\"%.2f\")\n",
    "df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for res in [\"WER\", \"CER\"]:\n",
    "    fig, ax = plt.subplots(2, 3, figsize=(12, 8), sharey=True)\n",
    "    names = [c if c != \"MSA\" else \"All\" for c in df_full.columns ]\n",
    "    for i, name in enumerate(list(names)):\n",
    "        k = int(i%3)\n",
    "        i = int(i/3)\n",
    "        df_new = pd.concat([df_finetune.loc[res].loc[[name]], df_scratch.loc[res].loc[[name]]], keys=[\"With\", \"Without\"], names=[\"Pre-Training\"])\n",
    "        df_new = df_new.T\n",
    "        df_new.plot(kind='bar', ax=ax[i][k], colormap='Set2', rot=45)\n",
    "        ax[i][k].legend().remove()\n",
    "        ax[i][k].set_title(f\"Dialect: {name}\")\n",
    "        ax[i][k].grid(True, axis='y')\n",
    "            \n",
    "    ax[0][0].set_ylabel(f\"{res} (%)\")\n",
    "    ax[1][0].set_ylabel(f\"{res} (%)\")\n",
    "    ax[0][0].legend(ncol=1, fancybox=True, shadow=True, title=\"Pre-Training\", loc=\"upper left\", labels=[\"With\", \"Without\"])\n",
    "    for i in range(3):\n",
    "        ax[1][i].set_xlabel(\"Dialect (Test set)\")\n",
    "        ax[0][i].set_xticklabels([])\n",
    "\n",
    "    plt.ylim(bottom=0)\n",
    "    fig.suptitle(\"Model performance on each test set with and without pre-training\", fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(f\"results/plots/ex_comparison_{res.lower()}.pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "scratch_dist = df_scratch.loc['WER'].drop('Whisper-Small')\n",
    "finetune_dist = df_finetune.loc['WER'].drop('Whisper-Small')\n",
    "\n",
    "mean_scratch, std_scratch = scratch_dist.mean(), scratch_dist.std()\n",
    "mean_finetune, std_finetune = finetune_dist.mean(), finetune_dist.std()\n",
    "\n",
    "z_value, p_value = stats.wilcoxon(scratch_dist, finetune_dist)\n",
    "test_df = pd.DataFrame({\n",
    "    'Mean Without': mean_scratch,\n",
    "    'Std Without': std_scratch,\n",
    "    'Mean With': mean_finetune,\n",
    "    'Std With': std_finetune,\n",
    "    'Z-statistic': z_value,\n",
    "    'p-value': p_value,\n",
    "}, index=mean_scratch.index)\n",
    "test_df = test_df.round(2)\n",
    "test_df['Mean Without'] = test_df['Mean Without'].astype(str) + ' ± ' + test_df['Std Without'].round(2).astype(str)\n",
    "test_df['Mean With'] = test_df['Mean With'].astype(str) + ' ± ' + test_df['Std With'].round(2).astype(str)\n",
    "test_df = test_df.drop(columns=['Std Without', 'Std With'])\n",
    "test_df.rename(columns={'Mean Without': 'Without pre-training', 'Mean With': 'With pre-training'}, inplace=True)\n",
    "test_df.to_latex(\"results/wilcoxon.tex\", bold_rows=True, caption=\"Wilcoxon results comparing the performance of models with and without pre-training\", label=\"tab:wilcoxon\", float_format=\"%.2f\")\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scratch_all_dist = scratch_dist.values.flatten()\n",
    "finetune_all_dist = finetune_dist.values.flatten()\n",
    "z_value, p_value = stats.wilcoxon(scratch_all_dist, finetune_all_dist)\n",
    "print(f\"Z-value: {z_value:.2f}, p-value: {p_value:.2f}\")\n",
    "print(f\"Mean without pre-training: {scratch_all_dist.mean():.2f}, Mean with pre-training: {finetune_all_dist.mean():.2f}\")\n",
    "print(f\"Standard deviation without pre-training: {scratch_all_dist.std():.2f}, Standard deviation with pre-training: {finetune_all_dist.std():.2f}\")\n",
    "print(f\"Degrees of freedom: {finetune_all_dist.size - 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "import matplotlib\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "cmap = matplotlib.colormaps['Set2']\n",
    "colors = [cmap(i) for i in range(2)]\n",
    "\n",
    "scratch_dist.plot(kind='box', ax=ax, positions=np.arange(0, len(scratch_dist)*2, 2), showmeans=True, meanline=True, patch_artist=True, boxprops=dict(facecolor=colors[0]), label=\"Without\")\n",
    "finetune_dist.plot(kind='box', ax=ax, positions=[i-0.5 for i in np.arange(1, len(scratch_dist)*2, 2)], showmeans=True, meanline=True, patch_artist=True, boxprops=dict(facecolor=colors[1]), label=\"With\")\n",
    "ax.set_xticks(np.arange(0.5, len(scratch_dist)*2, 2))\n",
    "ax.set_xticklabels(scratch_dist.T.index)\n",
    "ax.set_ylabel(\"WER (%)\")\n",
    "ax.set_xlabel(\"Dialect\")\n",
    "ax.legend([\"Without\", \"With\"], title=\"Pre-Training\")\n",
    "plt.title(\"Model performance on each test set without pre-training\", fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.grid(axis='y')\n",
    "\n",
    "plt.legend(loc='upper right',\n",
    "            ncol=1, fancybox=True, shadow=True, title=\"Pre-Training\", labels=[\"Without\", \"With\"], handles=[mpatches.Patch(facecolor=colors[0], label=\"Without\"), mpatches.Patch(facecolor=colors[1], label=\"With\")])\n",
    "plt.savefig(\"results/plots/ex_boxplot_within.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 3: Dialectal Arabic Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_finetune = load_results(\"results/ex_finetune\", \"finetune\")\n",
    "df_scratch = load_results(\"results/ex_scratch\", \"dialect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dialect in [\"Egyptian\", \"Levantine\", \"Gulf\", \"Iraqi\", \"Maghrebi\"]:\n",
    "    new_df = df_finetune.loc[\"WER\"].drop(columns=[\"MSA\", dialect]).loc[dialect]\n",
    "    print(dialect, new_df.mean().round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cm(df, metric, pretraining):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "    cm_df = df.drop(\"All\", level=\"model\")\n",
    "    cm_df = cm_df.drop(\"Whisper-Small\", level=\"model\")\n",
    "    cm_df = cm_df.drop(columns=[\"MSA\"])\n",
    "    cm_df.columns = list(cm_df.loc[\"WER\"].index)\n",
    "\n",
    "    sns.heatmap(cm_df.loc[metric], annot=True, cmap='viridis_r', fmt='g', cbar=True, ax=ax)\n",
    "    plt.title(f\"Model performance in {metric} (%) against each dialect {pretraining} pre-training\")\n",
    "    plt.ylabel(\"Dialect fine-tuned on\")\n",
    "    plt.xlabel(\"Dialect tested on\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"results/plots/ex_{'finetune' if pretraining == 'with' else 'scratch'}_cm_{metric.lower()}.pdf\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cm(df_finetune, \"WER\", \"with\")\n",
    "plot_cm(df_finetune, \"CER\", \"with\")\n",
    "plot_cm(df_scratch, \"WER\", \"without\")\n",
    "plot_cm(df_scratch, \"CER\", \"without\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(df, title, filename):\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(15, 10), sharex=True, sharey=False)\n",
    "    for i, res in enumerate([\"WER\", \"CER\"]):\n",
    "        axes = df.loc[res].plot(kind='bar', ax=ax[i], colormap='Set2', rot=0)\n",
    "        ax[i].set_ylabel(f\"{res} (%)\")\n",
    "        ax[i].set_ylim(0, 120 + 40 * (i == 0))\n",
    "        ax[i].set_yticks(np.arange(0, 120 + 40 * (i == 0), 20))\n",
    "        ax[i].grid(axis='y')\n",
    "        for g in axes.containers:\n",
    "            g_label = g.get_label()\n",
    "            for i, child in enumerate(g.get_children()):\n",
    "                bar_name = df.loc[res].iloc[i].name\n",
    "                if bar_name == g_label:\n",
    "                    child.set(hatch='//')\n",
    "\n",
    "    ax[1].set_xlabel(\"Dialect(s) trained on\")\n",
    "    ax[0].legend(loc='upper right', bbox_to_anchor=(1.1, 1.02), ncol=1, fancybox=True, shadow=True, title=\"Test set\")\n",
    "    ax[1].legend().remove()\n",
    "    fig.suptitle(title, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(df_scratch, \"Model performance on each test set without pre-training\", \"results/plots/ex_scratch.pdf\")\n",
    "plot_results(df_finetune, \"Model performance on each test set with pre-training\", \"results/plots/ex_finetune.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(15, 10), sharex=True, sharey=False)\n",
    "cmap = matplotlib.colormaps['Set2']\n",
    "colors = [cmap(i) for i in range(2)]\n",
    "dfs = []\n",
    "\n",
    "width = 0.4\n",
    "for i, res in enumerate([\"WER\", \"CER\"]):\n",
    "    multiplier = 0\n",
    "    all_res = dict(df_finetune.loc[res].loc[\"All\"].items())\n",
    "    x = np.arange(len(all_res.keys()))\n",
    "    y1 = [all_res[k] for k in all_res.keys()]\n",
    "    y2 = [df_finetune.loc[res].loc[k][k] for k in all_res.keys() if k != \"MSA\"] + [df_trainsize.loc[\"100%\"].loc[res]]\n",
    "    df_diff = pd.DataFrame({\"Dialect-pooled\": y1, \"Dialect-specific\": y2}, index=all_res.keys()).round(2)\n",
    "    mae = np.mean(np.abs(np.array(y1) - np.array(y2)))\n",
    "    print(f\"Mean absolute error between dialect-pooled and dialect-specific models for {res}: {mae:.2f}\")\n",
    "\n",
    "    ax[i].bar(x-0.2 + multiplier, y1, width, label=f\"Dialect-pooled\", color=colors[0])\n",
    "    ax[i].bar(x+0.2 + multiplier, y2, width, label=f\"Dialect-specific\", color=colors[1])\n",
    "    ax[i].grid(axis='y')\n",
    "    ax[i].set_ylim(bottom=0)\n",
    "    ax[i].set_ylabel(f\"{res} (%)\")\n",
    "    multiplier += 0.4\n",
    "    dfs.append(df_diff)\n",
    "\n",
    "# concatenate the dataframes with a multi index and save them to a latex file\n",
    "df_diff = pd.concat(dfs, keys=[\"WER\", \"CER\"], names=[\"Metric\"])\n",
    "df_diff.to_latex(f\"results/ex_comparison_dialectal.tex\", multirow=True, multicolumn=True, multicolumn_format=\"c\", bold_rows=True, caption=f\"Model performance comparison dialect-pooled vs dialect-specific for {res}\", label=f\"tab:ex_comparison_dialectal\", float_format=\"%.2f\")\n",
    "plt.xticks(x, [k for k in all_res.keys()])\n",
    "plt.xlabel(\"Test set\")\n",
    "plt.legend(title=\"Model\")\n",
    "fig.suptitle(\"Model performance comparison dialect-pooled vs dialect-specific\", fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"results/plots/ex_comparison_pooled.pdf\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
